#pagebreak()
= Data Management

== Data Sources
The system is designed to integrate data from multiple sources, reflecting real-world health data diversity:
1.  *Medical Imagery:* Chest X-ray images (e.g., PNG format). The initial dataset is based on the NIH Chest X-ray dataset, with a sample metadata file `sampled_nih_metadata.csv`.
2.  *Tabular Health History:* Metadata associated with the images, including patient demographics (age, gender), view position, and importantly, the ground truth `Finding Labels`.
3.  *Sensor Data:* Simulated time-series sensor data (e.g., heart rate, respiratory rate, SpO2, temperature, blood pressure) generated by `scripts/generate_simulated_sensor_data.py` and stored in `simulated_sensor_data.csv`. This data is linked to specific studies (images).

== Data Ingestion Pipeline
The ingestion process is initiated through the `data_ingestion_service`:
1.  A batch ingestion script (`scripts/run_batch_ingestion.py`) can be used to simulate the arrival of new patient data. This script reads image file paths, NIH metadata, and sensor data, then sends them to the `/ingest/batch/` endpoint of the Data Ingestion Service (via the API Gateway).
2.  The `data_ingestion_service`:
    a.  Uploads raw image files to the `raw-images` bucket in MinIO.
    b.  Uploads raw NIH metadata (for the batch) and splits/uploads sensor data per study to `raw-tabular` and `raw-sensor-data-per-study` buckets respectively.
    c.  For each study (image index):
        i.  Creates or retrieves patient information in the `patient_data_service`.
        ii. Creates or updates study information in the `patient_data_service`, linking the patient and storing paths to the raw data in MinIO.
        iii. Asynchronously calls the `image_preprocessing_service` to process the raw image.
        iv. Asynchronously calls the `tabular_preprocessing_service` to process both the NIH metadata portion relevant to the study and the specific sensor data for that study.

// Remember to create this diagram and place it in the images folder
#figure(
  image("images/data_ingestion_service.png", width: 100%),
  caption: [Data Ingestion Flow Diagram. Details the sequence from batch ingestion to storage and preprocessing triggers.]
)

== Data Preprocessing
Preprocessing is handled by specialized services:
1.  *Image Preprocessing (`image_preprocessing_service`):*
    - Fetches raw image from MinIO.
    - Applies transformations: resize to a fixed size (e.g., 224x224), converts to tensor, normalizes using pre-defined mean and standard deviation values.
    - Passes the preprocessed image through a pre-trained CNN (e.g., ResNet50) to extract a feature vector.
    - Saves the feature vector (tensor) to the `processed-image-features` bucket in MinIO.
    - Updates the study record in `patient_data_service` with the MinIO path of the features.
2.  *Tabular Preprocessing (`tabular_preprocessing_service`):*
    - *NIH Metadata (`nih_processor.py`):*
        - Fetches relevant study metadata from MinIO (or directly if passed).
        - Cleans data (e.g., 'Patient Age' string to integer).
        - Applies One-Hot Encoding to categorical columns (`NIH_CATEGORICAL_COLS`).
        - Applies Standard Scaling to numerical columns (`NIH_NUMERICAL_COLS`).
        - Saves the processed features (NumPy array) to `processed-tabular-nih-features` MinIO bucket.
    - *Sensor Data (`sensor_processor.py`):*
        - Fetches raw sensor data CSV for the study from MinIO.
        - Aggregates time-series data into features using operations like mean, std, min, max for defined columns (`SENSOR_COLUMNS_TO_AGGREGATE`, `SENSOR_AGGREGATIONS`).
        - Saves the processed features (NumPy array) to `processed-sensor-features` MinIO bucket.
    - Updates the study record in `patient_data_service` with MinIO paths for both types of tabular features.

// Remember to create this diagram and place it in the images folder
#figure(
  image("images/image_preprocessing.png", width: 40%),
  caption: [Detailed diagram for Image Preprocessing flow.]
)


== Data Storage
The system utilizes two primary storage solutions:
1.  *MinIO:* An S3-compatible object storage service. It is used for storing:
    - Raw uploaded data: images, tabular CSVs (NIH metadata, sensor data).
    - Processed data: extracted image features, processed NIH tabular features, processed sensor features.
    - Trained machine learning models and associated artifacts (encoders, scalers, label binarizers).
    - Buckets are defined in `docker-compose.yml` and service configurations (e.g., `BUCKET_RAW_IMAGES`, `BUCKET_PROCESSED_IMAGE_FEATURES`, `BUCKET_MODELS_STORE`).
2.  *PostgreSQL:* A relational database used by the `patient_data_service`.
    - Stores structured metadata about patients and their studies.
    - Includes paths/keys to the actual data objects stored in MinIO, linking the metadata with the binary data.
    - Schema is defined in `patient_data_service/app/models.py` and managed by Alembic migrations.